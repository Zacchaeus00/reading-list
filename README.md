# reading-list

[Revisiting Transformer-based Models for Long Document Classification](https://arxiv.org/abs/2204.06683) 2022

[HIBERT: Document Level Pre-training of Hierarchical Bidirectional Transformers for Document Summarization](https://arxiv.org/abs/1905.06566) 2019

[Hierarchical Transformers Are More Efficient Language Models](https://arxiv.org/abs/2110.13711) 2021

[Hierarchical Transformers for Multi-Document Summarization](https://arxiv.org/abs/1905.13164) ACL 2019
