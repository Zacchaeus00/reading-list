# reading-list

[Revisiting Transformer-based Models for Long Document Classification](https://arxiv.org/abs/2204.06683) 2022

[HIBERT: Document Level Pre-training of Hierarchical Bidirectional Transformers for Document Summarization](https://arxiv.org/abs/1905.06566) 2019

[Hierarchical Transformers Are More Efficient Language Models](https://arxiv.org/abs/2110.13711) 2021

[Hierarchical Transformers for Multi-Document Summarization](https://arxiv.org/abs/1905.13164) ACL 2019

[H-Transformer-1D: Fast One-Dimensional Hierarchical Attention for Sequences](https://arxiv.org/abs/2107.11906) ACL 2021
